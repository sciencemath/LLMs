{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797f5a04-174e-4acf-a0d4-76c25cf5e1c8",
   "metadata": {},
   "source": [
    "## Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f640edc-58d5-4a83-a4b0-c4d471731b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT model and classification head are trainable, and updated jointly (learn from one another)\n",
    "# Fine-tuning a pretrained BERT model for sentiment anaylsis\n",
    "from datasets import load_dataset\n",
    "\n",
    "tomatoes = load_dataset(\"rotten_tomatoes\")\n",
    "train_data, test_data = tomatoes[\"train\"], tomatoes[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac579cc-faae-4440-a1e5-d5d09ac778da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_id = \"bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18134e8-39b2-4e70-a153-99c387061295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e188acaf-9572-421c-abf4-b3e564661a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logged metrics, detects overfitting\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    f1 = f1_score(labels, predictions, average=\"binary\")\n",
    "    \n",
    "    #load_f1 = load_metric(\"f1\")\n",
    "    #f1 = load_f1.compute(predictions=predictions, references=labels, average=\"binary\")[\"f1\"]\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9b3607e-6eed-4827-bfd5-cec95ed6c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dt/x2hsbqvj7bsd0klwc9f8ff8m0000gn/T/ipykernel_33148/1447919989.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba72a559-6bde-4672-b2a7-943c69372a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8463835716247559,\n",
       " 'eval_model_preparation_time': 0.0182,\n",
       " 'eval_f1': 0.6666666666666666,\n",
       " 'eval_runtime': 5.6318,\n",
       " 'eval_samples_per_second': 189.282,\n",
       " 'eval_steps_per_second': 11.897}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984387e-7f94-4d3a-9198-f886c2163ceb",
   "metadata": {},
   "source": [
    "### Freezing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f3fb2cd-2973-491d-bf01-a00dcd4b668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ac38d2f-3552-42e9-8faa-3ce8ade334af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "# check what we might want to freeze\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "053f1372-16a7-4e13-bb90-a5e59e4d4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze everything except classification head\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"classifier\"):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6bf8528-a6ca-468e-9729-ea565373d256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dt/x2hsbqvj7bsd0klwc9f8ff8m0000gn/T/ipykernel_33148/1482939272.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 00:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.686100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.6863191511747095, metrics={'train_runtime': 51.1776, 'train_samples_per_second': 166.675, 'train_steps_per_second': 10.434, 'total_flos': 227605451772240.0, 'train_loss': 0.6863191511747095, 'epoch': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1703d-7abb-4ece-9570-982c95081993",
   "metadata": {},
   "source": [
    "## ^ training is much faster (only training the classification head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3abed8b4-6516-449a-9ac8-5c9210e4cb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6762992143630981,\n",
       " 'eval_f1': 0.6201966041108132,\n",
       " 'eval_runtime': 6.2526,\n",
       " 'eval_samples_per_second': 170.489,\n",
       " 'eval_steps_per_second': 10.716,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69afb98-ac09-46b7-920a-118305bd166d",
   "metadata": {},
   "source": [
    "### notice f1 score is low (0.62) lets freeze only the first 10 encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "461ff743-74ba-4f98-8805-4d0c88ea7b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f37fbc5-9b2b-4a9f-beea-4e7e544c7dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dt/x2hsbqvj7bsd0klwc9f8ff8m0000gn/T/ipykernel_33148/757739267.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 01:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.472287653090802, metrics={'train_runtime': 68.7508, 'train_samples_per_second': 124.071, 'train_steps_per_second': 7.767, 'total_flos': 227605451772240.0, 'train_loss': 0.472287653090802, 'epoch': 1.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder block 11 starts at index 165 freeze everything before\n",
    "for index, (name, param) in enumerate(model.named_parameters()):\n",
    "    if index < 165:\n",
    "        param.requires_grad = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "945ca0d1-1b79-4dcd-adf3-c18a94264fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4116963744163513,\n",
       " 'eval_f1': 0.8164435946462715,\n",
       " 'eval_runtime': 5.3717,\n",
       " 'eval_samples_per_second': 198.449,\n",
       " 'eval_steps_per_second': 12.473,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6324436f-4544-477b-9641-a2b8fb40e539",
   "metadata": {},
   "source": [
    "### ^ much better F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953d1ec-986a-457a-a8eb-889ca3263538",
   "metadata": {},
   "source": [
    "## Few-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0306ec8f-4671-44ba-95eb-1eed3c56d3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setfit\n",
      "  Downloading setfit-1.1.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: datasets>=2.15.0 in ./.venv/lib/python3.11/site-packages (from setfit) (2.21.0)\n",
      "Requirement already satisfied: sentence-transformers>=3 in ./.venv/lib/python3.11/site-packages (from sentence-transformers[train]>=3->setfit) (3.3.1)\n",
      "Requirement already satisfied: transformers>=4.41.0 in ./.venv/lib/python3.11/site-packages (from setfit) (4.47.1)\n",
      "Collecting evaluate>=0.3.0 (from setfit)\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: huggingface_hub>=0.24.0 in ./.venv/lib/python3.11/site-packages (from setfit) (0.27.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (from setfit) (1.6.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from setfit) (24.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets>=2.15.0->setfit) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from datasets>=2.15.0->setfit) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.11/site-packages (from datasets>=2.15.0->setfit) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets>=2.15.0->setfit) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets>=2.15.0->setfit) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.11/site-packages (from datasets>=2.15.0->setfit) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.11/site-packages (from datasets>=2.15.0->setfit) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets>=2.15.0->setfit) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.11/site-packages (from datasets>=2.15.0->setfit) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.15.0->setfit) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from datasets>=2.15.0->setfit) (3.11.11)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets>=2.15.0->setfit) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub>=0.24.0->setfit) (4.12.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (2.5.1)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.11/site-packages (from sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (1.13.1)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.11/site-packages (from sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (10.4.0)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in ./.venv/lib/python3.11/site-packages (from sentence-transformers[train]>=3->setfit) (1.2.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers>=4.41.0->setfit) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers>=4.41.0->setfit) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.11/site-packages (from transformers>=4.41.0->setfit) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->setfit) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->setfit) (3.5.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate>=0.20.3->sentence-transformers[train]>=3->setfit) (6.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.15.0->setfit) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.15.0->setfit) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.15.0->setfit) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.15.0->setfit) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.15.0->setfit) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.15.0->setfit) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.15.0->setfit) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.15.0->setfit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.15.0->setfit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.15.0->setfit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.15.0->setfit) (2024.12.14)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets>=2.15.0->setfit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets>=2.15.0->setfit) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.15.0->setfit) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (3.0.2)\n",
      "Downloading setfit-1.1.1-py3-none-any.whl (75 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate, setfit\n",
      "Successfully installed evaluate-0.4.3 setfit-1.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install setfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "950eb51f-0576-4e14-adb7-8931ed49e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import sample_dataset\n",
    "\n",
    "sampled_train_data = sample_dataset(tomatoes[\"train\"], num_samples=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a7be856-23ea-4df9-8f78-95b59254e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from setfit import SetFitModel\n",
    "\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e830eb68-7548-4545-afbd-aab7aad0fb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36a442246554a07bb1191529c1d5f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# contrastive learning\n",
    "from setfit import TrainingArguments as SetFitTrainingArguments\n",
    "from setfit import Trainer as SetFitTrainer\n",
    "\n",
    "args = SetFitTrainingArguments(\n",
    "    num_epochs=3,\n",
    "    num_iterations=20 # text pairs\n",
    ")\n",
    "args.eval_strategy = args.evaluation_strategy\n",
    "\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=sampled_train_data,\n",
    "    eval_dataset=test_data,\n",
    "    metric=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afca0a56-99a4-44ee-a8c3-5faeef98e981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 1280\n",
      "  Batch size = 16\n",
      "  Num epochs = 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 01:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc23099d44943d0b88fdb3bd7850e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49a1b6-1674-463e-a7c0-b548f5d4fbd2",
   "metadata": {},
   "source": [
    "### 1280 because 20 * 32 = 680 and 680 * 2 = 1280 sentence pairs from 32 labeled sentences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d121a-5f3a-4244-b8e1-c076f9de57a4",
   "metadata": {},
   "source": [
    "### Specify a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b0e44-ef02-4183-b3b8-0b52116d015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SetFitModel.from_pretrained(\n",
    "#     \"sentence-transformers/all-mpnet-base-v2\",\n",
    "#     use_differentiable_head=True,\n",
    "#     head_params={\"out_features\": num_classes}, # classes to predict\n",
    "# )\n",
    "# trainer = SetFitTrainer(model=model, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5aa98698-89a2-4fc6-b78a-96a008f4bffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4943a79bc64b4bb79244b276927c685e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8519230769230769}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f325bb2d-e4a4-4271-9e51-c013ad1c2ba4",
   "metadata": {},
   "source": [
    "## ^ Even better F1 score! On only 32 labeled documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658db644-99db-4f85-8249-d886b2b815b4",
   "metadata": {},
   "source": [
    "## In order to train on a specific domain, 3 step process:\n",
    "- use pretrain model\n",
    "- continue the training of the pretrain model with domain specific data\n",
    "- classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6224394d-9665-4b25-8526-1d3ad2b60e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_train = tokenized_train.remove_columns(\"label\")\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = tokenized_test.remove_columns(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3cf3fac-463c-4d00-ba6d-40f9304f40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15 # 15% a token is masked\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e1ec003-c5b6-4889-bed8-864297177ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dt/x2hsbqvj7bsd0klwc9f8ff8m0000gn/T/ipykernel_33148/3385921974.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91c7d742-d968-40ce-8f22-0c1317f6307d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5340' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5340/5340 46:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.602900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.377200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.978700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.966500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"mlm\")\n",
    "trainer.train()\n",
    "model.save_pretrained(\"mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90a33c6c-d160-4ab4-800c-6920cd7c1049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> What a horrible idea!\n",
      ">>> What a horrible dream!\n",
      ">>> What a horrible thing!\n",
      ">>> What a horrible day!\n",
      ">>> What a horrible thought!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
    "preds = mask_filler(\"What a horrible [MASK]!\")\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d79c420-5642-4572-b46a-ff3ade9fbb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> What a horrible movie!\n",
      ">>> What a horrible film!\n",
      ">>> What a horrible mess!\n",
      ">>> What a horrible story!\n",
      ">>> What a horrible comedy!\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"mlm\")\n",
    "preds = mask_filler(\"What a horrible [MASK]!\")\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0b3ca-0439-4e0c-ba00-80b46d852f27",
   "metadata": {},
   "source": [
    "## ^ biased with the model we fed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71923384-0b60-4362-ab33-2086c68b6120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mlm\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mlm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15a89d-76eb-4fb4-aa54-4dd06a9c455e",
   "metadata": {},
   "source": [
    "## Named-Entity Recognition (NER)\n",
    "(makes predicitions for individual tokens in a sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac136094-8729-441d-a3a7-5821dadfce5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "# lots of different datasets to choose from this one contains\n",
    "# different types of named entities (person, organization, location, misc, no entity)\n",
    "\n",
    "# Limit the training data to the first 1000 samples\n",
    "# train_subset = dataset[\"train\"].select(range(1000))\n",
    "\n",
    "# Create a new dataset with the limited training data\n",
    "# dataset = dataset.with_format(\"torch\")  # Adjust format if necessary\n",
    "# dataset[\"train\"] = train_subset\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbced362-4608-4de7-b317-01f4da823b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '401',\n",
       " 'tokens': ['Karol',\n",
       "  'Kucera',\n",
       "  '(',\n",
       "  'Slovakia',\n",
       "  ')',\n",
       "  'beat',\n",
       "  'Hicham',\n",
       "  'Arazi',\n",
       "  '(',\n",
       "  'Morocco',\n",
       "  ')',\n",
       "  '7-6',\n",
       "  '(',\n",
       "  '7-4',\n",
       "  ')'],\n",
       " 'pos_tags': [22, 22, 4, 22, 5, 37, 22, 22, 4, 22, 5, 11, 4, 11, 5],\n",
       " 'chunk_tags': [11, 12, 0, 11, 0, 21, 11, 12, 0, 11, 0, 3, 0, 11, 0],\n",
       " 'ner_tags': [1, 2, 0, 5, 0, 0, 1, 2, 0, 5, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[\"train\"][401]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5860b-0a16-4604-a762-fe35204407dc",
   "metadata": {},
   "source": [
    "### labels ^ ner_tags key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22cf24aa-8b5c-4e1d-a385-c2e8ed02dcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " 'B-PER': 1,\n",
       " 'I-PER': 2,\n",
       " 'B-ORG': 3,\n",
       " 'I-ORG': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-LOC': 6,\n",
       " 'B-MISC': 7,\n",
       " 'I-MISC': 8}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping person (PER), orginization (ORG), location (LOC) etc\n",
    "# B (beginning) or I (indside) two tokens are part of the same phrase, start of phrase B, folowed by I\n",
    "# example: first name (B) last name (I) was at location (B)\n",
    "label2id = {\n",
    "    \"0\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-ORG\": 3, \"I-ORG\": 4,\n",
    "    \"B-LOC\": 5, \"I-LOC\": 6, \"B-MISC\": 7, \"I-MISC\": 8\n",
    "}\n",
    "id2label = {index: label for label, index in label2id.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df8ac529-964d-4083-b4c7-d097b23b83b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e550c59d-bf8a-4956-b7e2-ffa65468e0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Ka',\n",
       " '##rol',\n",
       " 'Ku',\n",
       " '##cer',\n",
       " '##a',\n",
       " '(',\n",
       " 'Slovakia',\n",
       " ')',\n",
       " 'beat',\n",
       " 'Hi',\n",
       " '##cha',\n",
       " '##m',\n",
       " 'Ara',\n",
       " '##zi',\n",
       " '(',\n",
       " 'Morocco',\n",
       " ')',\n",
       " '7',\n",
       " '-',\n",
       " '6',\n",
       " '(',\n",
       " '7',\n",
       " '-',\n",
       " '4',\n",
       " ')',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer(example[\"tokens\"], is_split_into_words=True)[\"input_ids\"]\n",
    "sub_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "sub_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98fb3fa0-65f7-475b-8ea4-785bde30010b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478c76c357654d01a4144e0ccdd4491e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7148b3078a564f64b252e4df0750921c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7399a7b0ce6b4c4789ef53365fc407a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We need to make sure to properly label not words themselves but the tokens so that B can be followed by I if separated\n",
    "# we use -100 for special tokens ([SEP], [CLS])\n",
    "def align_labels(examples):\n",
    "    token_ids = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    labels = examples[\"ner_tags\"]\n",
    "\n",
    "    updated_labels = []\n",
    "    for index, label in enumerate(labels):\n",
    "        word_ids = token_ids.word_ids(batch_index=index)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx != previous_word_idx:\n",
    "                previous_word_idx = word_idx\n",
    "                label_ids.append(-100 if word_idx is None else label[word_idx])\n",
    "            elif word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                # label_ids.append(label[word_idx])\n",
    "                updated_label = label[word_idx]\n",
    "                if updated_label % 2 == 1:\n",
    "                    updated_label += 1\n",
    "                label_ids.append(updated_label)\n",
    "        updated_labels.append(label_ids)\n",
    "\n",
    "    token_ids[\"labels\"] = updated_labels\n",
    "    return token_ids\n",
    "\n",
    "tokenized = dataset.map(align_labels, batched=True, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3485ff1f-6f31-49b2-813e-083060674921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [1, 2, 0, 5, 0, 0, 1, 2, 0, 5, 0, 0, 0, 0, 0]\n",
      "Updated: [-100, 1, 2, 2, 2, 2, 0, 5, 0, 0, 1, 2, 2, 2, 2, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original: {example['ner_tags']}\")\n",
    "print(f\"Updated: {tokenized['train'][401]['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a90fa780-f024-4a14-87de-478bcdd74043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in ./.venv/lib/python3.11/site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in ./.venv/lib/python3.11/site-packages (from seqeval) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16217 sha256=be8c04594381af5ffdc6be46950b664f6a7d5820d7e8aaf2f01a032134772bff\n",
      "  Stored in directory: /Users/mathias/Library/Caches/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d886be59-477d-41ed-ad46-2dcf0c81d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance\n",
    "import evaluate\n",
    "import numpy as np\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=2)  # Shape: (num_samples, seq_length)\n",
    "\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Process each sequence in the batch\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        filtered_predictions = []\n",
    "        filtered_labels = []\n",
    "        for p, l in zip(prediction, label):\n",
    "            if l != -100:  # Ignore padding tokens\n",
    "                filtered_predictions.append(id2label[p])\n",
    "                filtered_labels.append(id2label[l])\n",
    "        true_predictions.append(filtered_predictions)  # Append the sequence\n",
    "        true_labels.append(filtered_labels)  # Append the sequence\n",
    "\n",
    "    # Compute metrics using seqeval\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48f69608-125f-46f1-bdd0-8528c88cae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "991ab47c-4b5c-4507-a3ef-2694a6c4cc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dt/x2hsbqvj7bsd0klwc9f8ff8m0000gn/T/ipykernel_41640/2095477416.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='878' max='878' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [878/878 04:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.050100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=878, training_loss=0.0476915733146233, metrics={'train_runtime': 265.1951, 'train_samples_per_second': 52.946, 'train_steps_per_second': 3.311, 'total_flos': 351240792638148.0, 'train_loss': 0.0476915733146233, 'epoch': 1.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e5f85f1-66f5-4594-9b67-260fd7211c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathias/Documents/research2025/LLMs/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1686190813779831,\n",
       " 'eval_f1': 0.9085269741190827,\n",
       " 'eval_accuracy': 0.9704542947637131,\n",
       " 'eval_runtime': 15.807,\n",
       " 'eval_samples_per_second': 218.448,\n",
       " 'eval_steps_per_second': 13.665,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "591ad2db-d2d8-4359-a912-ce9e39ce2abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': '0',\n",
       "  'score': 0.999423,\n",
       "  'index': 1,\n",
       "  'word': 'My',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity': '0',\n",
       "  'score': 0.9991417,\n",
       "  'index': 2,\n",
       "  'word': 'name',\n",
       "  'start': 3,\n",
       "  'end': 7},\n",
       " {'entity': '0',\n",
       "  'score': 0.9992617,\n",
       "  'index': 3,\n",
       "  'word': 'is',\n",
       "  'start': 8,\n",
       "  'end': 10},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9910926,\n",
       "  'index': 4,\n",
       "  'word': 'Mathias',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity': '0',\n",
       "  'score': 0.99933213,\n",
       "  'index': 5,\n",
       "  'word': ',',\n",
       "  'start': 18,\n",
       "  'end': 19},\n",
       " {'entity': '0',\n",
       "  'score': 0.9991381,\n",
       "  'index': 6,\n",
       "  'word': 'nice',\n",
       "  'start': 20,\n",
       "  'end': 24},\n",
       " {'entity': '0',\n",
       "  'score': 0.9992908,\n",
       "  'index': 7,\n",
       "  'word': 'to',\n",
       "  'start': 25,\n",
       "  'end': 27},\n",
       " {'entity': '0',\n",
       "  'score': 0.9992685,\n",
       "  'index': 8,\n",
       "  'word': 'meet',\n",
       "  'start': 28,\n",
       "  'end': 32},\n",
       " {'entity': '0',\n",
       "  'score': 0.9993894,\n",
       "  'index': 9,\n",
       "  'word': 'you',\n",
       "  'start': 33,\n",
       "  'end': 36},\n",
       " {'entity': '0',\n",
       "  'score': 0.99974746,\n",
       "  'index': 10,\n",
       "  'word': '.',\n",
       "  'start': 36,\n",
       "  'end': 37},\n",
       " {'entity': '0',\n",
       "  'score': 0.9996001,\n",
       "  'index': 11,\n",
       "  'word': 'Let',\n",
       "  'start': 38,\n",
       "  'end': 41},\n",
       " {'entity': '0',\n",
       "  'score': 0.9992137,\n",
       "  'index': 12,\n",
       "  'word': '##s',\n",
       "  'start': 41,\n",
       "  'end': 42},\n",
       " {'entity': '0',\n",
       "  'score': 0.9996642,\n",
       "  'index': 13,\n",
       "  'word': 'go',\n",
       "  'start': 43,\n",
       "  'end': 45},\n",
       " {'entity': '0',\n",
       "  'score': 0.99954695,\n",
       "  'index': 14,\n",
       "  'word': 'to',\n",
       "  'start': 46,\n",
       "  'end': 48},\n",
       " {'entity': '0',\n",
       "  'score': 0.9868254,\n",
       "  'index': 15,\n",
       "  'word': 'the',\n",
       "  'start': 49,\n",
       "  'end': 52},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.5220272,\n",
       "  'index': 16,\n",
       "  'word': 'Arcade',\n",
       "  'start': 53,\n",
       "  'end': 59}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving model to use for pipeline inference, allows us to manually check to see if satisfied with output\n",
    "from transformers import pipeline\n",
    "\n",
    "trainer.save_model(\"ner_model\")\n",
    "\n",
    "# run inference\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"ner_model\",\n",
    ")\n",
    "token_classifier(\"My name is Mathias, nice to meet you. Lets go to the Arcade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316495fd-381e-4ca8-ba0c-3ed7c5f7b140",
   "metadata": {},
   "source": [
    "### correctly identified a person!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a977d05a-8201-46ad-84d4-e13cdc986505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
