{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ef754c-5e03-489d-b785-29f8a3e58e62",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ba8345-7d20-4434-8669-4c0b69a33992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76633b3d84034b98867a735a7f0ea77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08759af5-5815-4cdf-9cf1-4008234c439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did Elon Musk start a company to make pizza? Because he wanted to take the \"pizza\" to the next level with his SpaceX and Tesla innovations!\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a funny joke about Elon Musk.\"}\n",
    "]\n",
    "\n",
    "output = pipe(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb96e678-7d02-4240-9405-9edacd332c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Create a funny joke about Elon Musk.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# I don't get the previous joke\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04040711-27ad-434e-919a-1d4861f671d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did Elon Musk bring a ladder to the boring meeting?\n",
      "\n",
      "Because he wanted to make sure the ideas wouldn't fall flat!\n"
     ]
    }
   ],
   "source": [
    "output = pipe(messages, do_sample=True, temperature=1)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa32db20-37f9-4e67-afe7-1fea47568e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did Elon Musk invent a reusable rockets?\n",
      "\n",
      "\n",
      "Because he wanted to ensure that more than just the moon stays in orbit without an awkward apology to the Earth.\n"
     ]
    }
   ],
   "source": [
    "output = pipe(messages, do_sample=True, top_p=1)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a945515f-5dc6-467e-aa5a-fef7e478358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "A 'Ocarina' is a type of clay flute. An example of a sentences that uses the word Ocarina is:<|end|>\n",
      "<|assistant|>\n",
      "I have a Ocarina that someone special gave me as a gift. I love to play it at home.<|end|>\n",
      "<|user|>\n",
      "'Song of Time' is played by this instrument. Another song that is played by the Ocarina is:<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "one_shot_prompt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"A 'Ocarina' is a type of clay flute. An example of a sentences that uses the word Ocarina is:\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I have a Ocarina that someone special gave me as a gift. I love to play it at home.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"'Song of Time' is played by this instrument. Another song that is played by the Ocarina is:\"\n",
    "    }\n",
    "]\n",
    "print(tokenizer.apply_chat_template(one_shot_prompt, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfe41e1d-0f85-46bd-87e9-5c4d668d0992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Ocarina is a unique and ancient musical instrument, often made from clay and known for its distinctive, haunting sound. It has been used in various cultures around the world for centuries. The 'Song of Time' is a fictional piece that could be played on an Ocarina, symbolizing the passage of time and the transient nature of life.\n",
      "\n",
      "Another song that is played by the Ocarina is \"The Wind's Whisper.\" This imaginary piece could be a beautiful melody that captures the gentle, soothing sound of the wind as it rustles through the leaves of a tree. The Ocarina's ability to produce a wide range of tones and its ethereal quality make it an ideal instrument for conveying the delicate and ephemeral qualities of the wind.\n",
      "\n",
      "In a more real-world context, the Ocarina has been featured in various musical compositions and performances. For example, the Ocarina has been used in the soundtrack of the video game \"The Legend of Zelda: Ocarina of Time,\" where it plays a crucial role in the game's storyline and gameplay mechanics. The Ocarina's distinctive sound adds a magical and whimsical element to the game, enhancing the overall experience for players.\n",
      "\n",
      "In addition to video games, the Ocarina has also been used in various musical genres, including classical, folk, and world music. Its unique sound and versatility have made it a popular choice for musicians and composers seeking to create a distinctive and memorable musical experience.\n",
      "\n",
      "In conclusion, the Ocarina is a fascinating and versatile instrument that has been used in various musical compositions and performances throughout history. Its ability to produce a wide range of tones and its ethereal quality make it an ideal instrument for conveying the passage of time, the gentle whispers of the wind, and the magic of the world around us.\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(one_shot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "204b99de-b358-495e-8f2d-01c5b2015672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Name: \"Blade of the Fallen\"\n",
      "\n",
      "Slogan: \"Cut through your enemies with unparalleled power, but beware the price of wielding the Fallen's curse.\"\n"
     ]
    }
   ],
   "source": [
    "product_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a name and slogan for a new sword for Elden Ring that injures you but does great damage.\"}\n",
    "]\n",
    "outputs = pipe(product_prompt)\n",
    "product_description = outputs[0][\"generated_text\"]\n",
    "print(product_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7720134-3816-4cb0-a4e9-f2abb4d53790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"Blade of the Fallen\" offers unmatched strength in battle, ensuring victory over foes with its legendary edge.\n"
     ]
    }
   ],
   "source": [
    "use_prompt = [\n",
    "    {\"role\": \"user\", \"content\": f\"Generate a very short reason why to use this sword: {product_description}\"}\n",
    "]\n",
    "outputs = pipe(use_prompt)\n",
    "usage_pitch = outputs[0][\"generated_text\"]\n",
    "print(usage_pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c2f69f7-1e7b-45ce-bde7-619095678f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The gardener initially planted 18 rows * 7 flowers/row = 126 flowers. After the wind, there are 126 - 36 = 90 flowers left in the garden.\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"A bakery made 15 cakes in the morning. In the afternoon, they baked 12 more cakes. Later, they packed these cakes into boxes, with each box holding 3 cakes. How many boxes did the bakery use?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The bakery has a total of 15 + 12 = 27 cakes when putting them in boxes we have 27 / 3 = 9 boxes total\"},\n",
    "    {\"role\": \"user\", \"content\": \"A gardener planted 18 rows of flowers in a garden. Each row had 7 flowers. After a windy day, 36 flowers were blown away. How many flowers are still in the garden?\"}\n",
    "]\n",
    "outputs = pipe(chain_of_thought_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46cbab72-100a-4bef-996f-e88a9e4b4a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1: Determine the total number of cupcakes Sarah initially has.\n",
      "Sarah has 48 cupcakes.\n",
      "\n",
      "Step 2: Calculate the number of cupcakes to be set aside for decoration.\n",
      "Sarah wants to set aside 8 cupcakes for decoration.\n",
      "\n",
      "Step 3: Subtract the cupcakes set aside from the total number of cupcakes.\n",
      "48 cupcakes - 8 cupcakes = 40 cupcakes remaining.\n",
      "\n",
      "Step 4: Divide the remaining cupcakes equally among the 4 trays.\n",
      "40 cupcakes ÷ 4 trays = 10 cupcakes per tray.\n",
      "\n",
      "After setting aside the 8 cupcakes for decoration, there will be 10 cupcakes on each tray.\n"
     ]
    }
   ],
   "source": [
    "# added Let's think step-by-step at the end (prime reasoning)\n",
    "zeroshot_cot_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Sarah is preparing for a party. She buys 48 cupcakes. She wants to arrange them equally on 4 trays. After setting up the trays, she realizes she wants to set aside 8 cupcakes for a special decoration. How many cupcakes will be on each tray after setting aside the 8 cupcakes? Let's think step-by-step.\"}\n",
    "]\n",
    "outputs = pipe(zeroshot_cot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e66f28b8-6d9e-4d8d-b1c9-2812584c955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Expert 1:\n",
      "Step 1: I will start by assuming that the person has 1 apple initially.\n",
      "\n",
      "Step 2: The person eats 1 apple, so we subtract 1 from the initial amount.\n",
      "\n",
      "Step 3: After eating 1 apple, the person has 0 apples left.\n",
      "\n",
      "Expert 2:\n",
      "Step 1: I will consider the initial condition, which is that the person has 1 apple.\n",
      "\n",
      "Step 2: The person consumes 1 apple, so we need to subtract 1 from the initial amount.\n",
      "\n",
      "Step 3: After eating 1 apple, the person has 0 apples remaining.\n",
      "\n",
      "Expert 3:\n",
      "Step 1: I will assume that the person starts with 1 apple.\n",
      "\n",
      "Step 2: The person eats 1 apple, so we subtract 1 from the initial amount.\n",
      "\n",
      "Step 3: After eating 1 apple, the person has 0 apples left.\n",
      "\n",
      "All experts agree that if a person has 1 apple and eats 1 apple, they will have 0 apples left.\n"
     ]
    }
   ],
   "source": [
    "zeroshot_tree_of_thought_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave. The question is 'if I have 1 apple then ate 1 apple how many apples do I have left?' Make sure to discuss the results\"}\n",
    "]\n",
    "outputs = pipe(zeroshot_tree_of_thought_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805de7a0-0277-46cf-abf8-cd56a9990a59",
   "metadata": {},
   "source": [
    "## guiding the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4de7771-6f33-44a7-8352-ee7edb1bb827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```json\n",
      "{\n",
      "  \"name\": \"Aria Stormbringer\",\n",
      "  \"class\": \"Warrior\",\n",
      "  \"race\": \"Human\",\n",
      "  \"level\": 10,\n",
      "  \"attributes\": {\n",
      "    \"strength\": 18,\n",
      "    \"dexterity\": 12,\n",
      "    \"constitution\": 16,\n",
      "    \"intelligence\": 8,\n",
      "    \"wisdom\": 10,\n",
      "    \"charisma\": 14\n",
      "  },\n",
      "  \"skills\": {\n",
      "    \"melee\": 15,\n",
      "    \"ranged\": 10,\n",
      "    \"magic\": 5,\n",
      "    \"stealth\": 12,\n",
      "    \"acrobatics\": 10,\n",
      "    \"animal_handling\": 8\n",
      "  },\n",
      "  \"equipment\": {\n",
      "    \"weapon\": \"Two-handed Axe\",\n",
      "    \"armor\": \"Chainmail Hauberk\",\n",
      "    \"shield\": \"Warhammer\",\n",
      "    \"accessories\": [\n",
      "      \"Warrior's Talisman\",\n",
      "      \"Leather Boots\",\n",
      "      \"Woolen Cloak\"\n",
      "    ]\n",
      "  },\n",
      "  \"background\": \"Aria grew up in a small village on the outskirts of a large city. She was always fascinated by the stories of great warriors and adventurers who roamed the lands. When she was just a child, she witnessed a group of bandits attacking her village. She fought bravely and helped her village defeat the bandits. Since then, she has dedicated her life to becoming a skilled warrior and protecting those in need.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "zeroshot_prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a character profile for an RPG game in JSON format.\"}\n",
    "]\n",
    "outputs = pipe(zeroshot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2584c8e-1551-459d-8a4b-a74e6e72fb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "    \"description\": \"A cunning rogue with a mysterious past, skilled in stealth and deception.\",\n",
      "    \"name\": \"Shadowcloak\",\n",
      "    \"armor\": \"Leather Hood\",\n",
      "    \"weapon\": \"Dagger\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "one_shot_template = \"\"\"Create a short character profile for an RPG game. Make sure to only use this format:\n",
    "{\n",
    "    \"description\": \"A SHORT DESCRIPTION\",\n",
    "    \"name\": \"THE CHARACTER'S NAME\",\n",
    "    \"armor\": \"ONE PIECE OF ARMOR\",\n",
    "    \"weapon\": \"ONE OR MORE WEAPONS\"\n",
    "}\n",
    "\"\"\"\n",
    "one_shot_prompt = [\n",
    "    {\"role\": \"user\", \"content\": one_shot_template}\n",
    "]\n",
    "outputs = pipe(one_shot_prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bec83e7-7177-416a-ab72-66b326bf05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "#del model, tokenizer, pipe\n",
    "\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a4a66e7-2ea7-4cae-9823-7663f2b53e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.6.tar.gz (66.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./.venv/lib/python3.11/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in ./.venv/lib/python3.11/site-packages (from llama-cpp-python) (3.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel done\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.6-cp311-cp311-macosx_15_0_arm64.whl size=3489031 sha256=e1d1a27da5e748b2af56d8e84acb0a005c119f079a8ae1c79fbb2ce6a5455dd4\n",
      "  Stored in directory: /Users/mathias/Library/Caches/pip/wheels/e8/96/d2/acfb576f7a58ef0580e2fec8096e5eefd17cc356017089337b\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce730ea0-a76e-480e-a003-7dea64d9510d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3304b1d2212f4d2a887d4225f95540f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phi-3-mini-4k-instruct-fp16.gguf:   0%|          | 0.00/7.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp.llama import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "    filename=\"*fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=2048,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "187b0fef-61a7-48b1-aeda-af4b630b7bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Create a character in Elden Ring in JSON format.\"},\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=0\n",
    ")['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8a9f32e-bb60-411b-91db-6cc7c00da2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"character\": {\n",
      "        \"name\": \"Eldrin the Wanderer\",\n",
      "        \"class\": \"Ranger\",\n",
      "        \"race\": \"Human\",\n",
      "        \"attributes\": {\n",
      "            \"strength\": 12,\n",
      "            \"dexterity\": 18,\n",
      "            \"constitution\": 14,\n",
      "            \"intelligence\": 10,\n",
      "            \"wisdom\": 12,\n",
      "            \"charisma\": 11\n",
      "        },\n",
      "        \"skills\": [\n",
      "            {\n",
      "                \"name\": \"Bow Mastery\",\n",
      "                \"level\": 5,\n",
      "                \"description\": \"Improves accuracy and damage with bows and crossbows.\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Tracking\",\n",
      "                \"level\": 5,\n",
      "                \"description\": \"Enhances ability to track and follow enemies.\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Wilderness Survival\",\n",
      "                \"level\": 5,\n",
      "                \"description\": \"Increases ability to survive in the wilderness and find resources.\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Stealth\",\n",
      "                \"level\": 4,\n",
      "                \"description\": \"Improves ability to move silently and avoid detection.\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Nature's Blessing\",\n",
      "                \"level\": 3,\n",
      "                \"description\": \"Reduces damage taken from natural hazards and increases healing from nature's blessings.\"\n",
      "            }\n",
      "        ],\n",
      "        \"equipment\": [\n",
      "            {\n",
      "                \"name\": \"Longbow\",\n",
      "                \"type\": \"Bow\",\n",
      "                \"damage\": 15,\n",
      "                \"range\": 120\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Leather Armor\",\n",
      "                \"type\": \"Light Armor\",\n",
      "                \"defense\": 10,\n",
      "                \"durability\": 100\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Leather Boots\",\n",
      "                \"type\": \"Footwear\",\n",
      "                \"defense\": 5,\n",
      "                \"durability\": 100\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Wooden Shield\",\n",
      "                \"type\": \"Shield\",\n",
      "                \"defense\": 8,\n",
      "                \"durability\": 100\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Wooden Sword\",\n",
      "                \"type\": \"Weapon\",\n",
      "                \"damage\": 10,\n",
      "                \"range\": 1\n",
      "            }\n",
      "        ],\n",
      "        \"personality\": \"Eldrin is a solitary and adventurous ranger who seeks to explore the vast lands of Elden Ring. He values freedom and self-reliance, and is always ready to face any challenge that comes his way. Despite his solitary nature, he is a compassionate and honorable individual who will help those in need.\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_output = json.dumps(json.loads(output), indent=4)\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ae0b3-210c-4010-8a31-f89698213243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
